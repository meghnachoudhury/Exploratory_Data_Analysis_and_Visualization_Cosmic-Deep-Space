[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Exploratory Data Analysis and Visualization of JWST Photometric Data in the Cosmic Deep Space",
    "section": "",
    "text": "1 Introduction\nI chose this project because I have always been fascinated with how we have advanced as a civilization to a point where we can peer deep into the universe, beyond our small planet, and learn about elements beyond us. For that purpose, the James Webb Space Telescope, which is the largest telescope in space and was launched quite recently on December 25, 2021, conducts infrared astronomy and the data we receive as result helps us perform all kinds of analysis of the deep space. It has a very high resolution and sensitivity, and can capture objects that are too old, far away, or barely visible for the Hubble Telescope. This project aims to analyze the data from JWST photometric catalogs in NASA‚Äôs public archives, visually explore the change and uncertainties in observed brightness/colors of a galaxy with cosmic time, and to capture patterns based on data parameters like depth, missing values, spatial non-uniformity, for distinguishing real trends from surveys. I will test the completeness of JWST data as well in this experiment. For some context, here are some straightforward theories in astrophysics that I will be using to frame my project goals:\n\nDistance equals time (visually).\nBig bang predicts a consistent trend between distance and time, and we know that the cosmic speed limit is the speed of light.\nAs light travels through the expanding space, its wavelength is stretched to longer frequencies. This phenomenon is called a cosmological redshift.\nThe amount of ‚Äústretch‚Äù tells us how long ago light was emitted. (Note: Let us treat redshift as a time coordinate.)\nMathematically, this can be calculated in an isotropic Friedman-Lema√Ætre-Robertson-Walker universe as a(t0)/a(temit), where a(t) is the scale factor, and t0 and temit are the light parameters for emitted and observed light, respectively.\n\nBut, I will not focus on the mathematics, but rather the analysis and visualization of a cluster of galaxies. I want to show the light and time comparison of some of the data zones in the observable deep space, and try to visually capture the patterns.\nBased on the above root theories, I will aim to answer the following questions:\n\nDoes observed brightness change with time? Change of median apparent with redshift.\nDo galaxies evolve with time? Or is there a survey bias in the JWST dataset?\nIs there spatial non-uniformity that confirms survey quirks?\nCan color and size parameters tell stars apart from galaxies?",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "2¬† Data",
    "section": "",
    "text": "2.1 Description\nCode\nlibrary(tidyverse)\nlibrary(viridis)\nlibrary(hexbin)\nlibrary(naniar)\nlibrary(ggridges)\nlibrary(scales)\nlibrary(naniar)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(stringr)\nlibrary(hexbin)\nlibrary(broom)\nlibrary(tibble)\nI used a public data catalog called the UNCOVER DR3 SUPER which is used to describe the Abell 2744 field. It is collected by the UNCOVER collaboration using James Webb Space Telescope. I have tried to understand the data with some question prompts:\nChallenges with Missing values- In real world, the problems encountered with this kind of data is that some objects are too faint in some filters (brightness undetected), and there is a lot of measurement uncertainty, so they end up as missing values. Also at very high redshifts, certain filters drop because even in JWST, they don‚Äôt capture light from very old galaxies.\nThe significant variables in this data:\nIn the below plots I will analyze the most optimal filters for observing by comparing different filters with different sets of missing data.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "data.html#description",
    "href": "data.html#description",
    "title": "2¬† Data",
    "section": "",
    "text": "What is the data about? The Abell 2744 field is a huge galaxy cluster in the Sculptor Constellation known as the Pandora‚Äôs Cluster. DR3 Photometry has over 74000 sources that are publicly archived, and measures different sky positions or brightness(RA, Dec), redshifts (spectroscopic z_spec, photometric estimates with uncertainties like z025/z975 and fit quality z_phot_chi2), fluxes (f_ variables), errors (e_ variables), and AB magnitudes (m_ variables) across various filters. It combines NIRCam mosaics with point-spread-function-matched apertures. A PSF( Point Spread Function) is a 2D distribution of light in a telescopic focal plane. The data catalog is released as a FITS (binary table) and a mirrored CSV/ECSV, which I downloaded from the UNCOVER github repo. I will use the CSV for the EDAV implementation.\nWhy does it exist? We want a measurement (PSF-matched, multi-filter comparison) of sources so that we can study galaxies.\nWhat are we using the data for? To observe age through trends of brightness vs redshift, evolution of galaxies, the treatment of missing data, spatial non-uniformity, and a categorization of stars vs non stars.\n\n\n\n\nPosition in the sky- ra, dec\nRedshift and Uncertainty- spectroscopic redshift (z_spec), Uncertainty width (z025, z975), photo z fit check (z_phot_chi2),Derived redshift bins (z_bin).\nFilters - f variables\nErrors- e variables\nMagnitude (AB) - m variables.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "data.html#missing-value-analysis",
    "href": "data.html#missing-value-analysis",
    "title": "2¬† Data",
    "section": "2.2 Missing value analysis",
    "text": "2.2 Missing value analysis\n\n\nCode\ncsv_path &lt;- \"/Users/base/Desktop/Columbia Documents/Courswork/EDAV/Project/uncover_dr3_super_catalog.csv\"  \ngal_raw  &lt;- readr::read_csv(csv_path, show_col_types = FALSE)\nhead(gal_raw, width = 120)\n\n\n# A tibble: 6 √ó 156\n     id     x     y    ra   dec ebv_mw `faper_f277w+f356w+f444w`\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;                     &lt;dbl&gt;\n1     1 4170. 0.804  3.62 -30.5 0.0107                        NA\n2     2 4191. 2.50   3.62 -30.5 0.0107                        NA\n3     3 5174. 1.55   3.61 -30.5 0.0109                        NA\n4     4 5250. 2.68   3.61 -30.5 0.0109                        NA\n5     5 3820. 1.53   3.63 -30.5 0.0107                        NA\n6     6 4862. 2.49   3.62 -30.5 0.0109                        NA\n# ‚Ñπ 149 more variables: `eaper_f277w+f356w+f444w` &lt;dbl&gt;,\n#   `fauto_f277w+f356w+f444w` &lt;dbl&gt;, `w_f277w+f356w+f444w` &lt;dbl&gt;,\n#   f_f435w &lt;dbl&gt;, e_f435w &lt;dbl&gt;, w_f435w &lt;dbl&gt;, f_f606w &lt;dbl&gt;, e_f606w &lt;dbl&gt;,\n#   w_f606w &lt;dbl&gt;, f_f814w &lt;dbl&gt;, e_f814w &lt;dbl&gt;, w_f814w &lt;dbl&gt;, f_f070w &lt;dbl&gt;,\n#   e_f070w &lt;dbl&gt;, w_f070w &lt;dbl&gt;, f_f090w &lt;dbl&gt;, e_f090w &lt;dbl&gt;, w_f090w &lt;dbl&gt;,\n#   f_f105w &lt;dbl&gt;, e_f105w &lt;dbl&gt;, w_f105w &lt;dbl&gt;, f_f115w &lt;dbl&gt;, e_f115w &lt;dbl&gt;,\n#   w_f115w &lt;dbl&gt;, f_f125w &lt;dbl&gt;, e_f125w &lt;dbl&gt;, w_f125w &lt;dbl&gt;, ‚Ä¶\n\n\n\n\nCode\nz_candidates &lt;- c(\"z\", \"z_phot\", \"z_best\", \"z_eazy\", \"photoz\", \"z_spec\")\nz_col &lt;- intersect(z_candidates, names(gal_raw))[1]\nif (is.na(z_col)) stop(\"No redshift column found. Look for one of: \",\n                      paste(z_candidates, collapse = \", \"))\nmessage(\"Using redshift column: \", z_col)\ngal &lt;- gal_raw %&gt;%\n  mutate(\n#standardizing z\n    z = as.numeric(.data[[z_col]]),\n    z = ifelse(is.finite(z) & z &gt;= 0, z, NA_real_),\n#Redshift bins\n    z_bin = cut(\n      z,\n      breaks = c(0, 1, 2, 3, 4, 6, 10, Inf),\n      labels = c(\"[0,1)\",\"[1,2)\",\"[2,3)\",\"[3,4)\",\"[4,6)\",\"[6,10)\",\"[10,+)\"),\n      right  = FALSE\n    )\n  )\n\n\nLet us check if missing data increases as we go higher in redshift?\n\n\nCode\nmag_from_uJy &lt;- function(f) {\n  f &lt;- suppressWarnings(as.numeric(f))#if column is in character\n  out &lt;- rep(NA_real_, length(f))\n  idx &lt;- is.finite(f) & f &gt; 0#positive fluxes\n  out[idx] &lt;- 23.9 - 2.5 * log10(f[idx])\n  out\n}\n\nbands &lt;- c(\"f150w\",\"f200w\",\"f277w\",\"f356w\",\"f360w\",\n           \"f410w\",\"f430w\",\"f444w\",\"f460w\",\"f480w\")\nfor (b in bands) {\n  fcol &lt;- paste0(\"f_\", b)\n  mcol &lt;- paste0(\"m_\", b)\n  if (fcol %in% names(gal) && !(mcol %in% names(gal))) {\n    gal[[mcol]] &lt;- mag_from_uJy(gal[[fcol]])\n  }\n}\n#Redshift bins\ngal &lt;- gal %&gt;%\n  mutate(z_bin = cut(\n    z, breaks = c(0,1,2,3,4,6,10, Inf),\n    labels = c(\"[0,1)\",\"[1,2)\",\"[2,3)\",\"[3,4)\",\"[4,6)\",\"[6,10)\",\"[10,+)\"),\n    right = FALSE\n  ))\nmag_cols &lt;- intersect(paste0(\"m_\", bands), names(gal))\n\n#Missing data\nlibrary(dplyr); library(tidyr); library(ggplot2)\nmiss_long &lt;- gal %&gt;%\n  transmute(z_bin, across(all_of(mag_cols), ~ !is.finite(.x), .names = \"NA_{col}\")) %&gt;%\n  pivot_longer(-z_bin, names_to = \"band\", values_to = \"is_na\") %&gt;%\n  mutate(\n    band = sub(\"^NA_m_\", \"\", band),\n    band = toupper(sub(\"^f\", \"F\", sub(\"^m_\", \"\", band)))\n  ) %&gt;%\n  group_by(z_bin, band) %&gt;%\n  summarise(pct_missing = mean(is_na) * 100, .groups = \"drop\") %&gt;%\n  filter(!is.na(z_bin))\nmiss_hl &lt;- miss_long %&gt;% mutate(is_high = z_bin == \"[10,+)\")\nband_labels &lt;- toupper(sub(\"^f\", \"F\", bands))\nggplot(miss_hl, aes(band, pct_missing, fill = is_high)) +\n  geom_col(position = position_dodge(width = 0.7), width = 0.65) +\n  scale_x_discrete(limits = band_labels, drop = FALSE) + \n  scale_fill_manual(values = c(`FALSE` = \"grey80\", `TRUE` = \"steelblue\"),\n                    labels = c(\"Other bins\", \"High-z [10,+)\"),\n                    name = NULL) +\n  labs(title = \"Percent missing by band (highlighting highest-z bin)\",\n       x = \"Filter\", y = \"% Missing\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 40, hjust = 1, face = \"bold\"))\n\n\n\n\n\n\n\n\n\nObservations:\n1.The blue bars show percentage of missing data at high redshift, and the grey bars show percentage of missing data at lower redshift. This fits perfectly with the astrological concept of galaxies becoming harder to observe with increasing redshift (cosmic time).\n2.The highlighted bars show higher missing data for the shorter wavelength JWST filters like F150W and F200W with the highest z redshift.\n3.For some filters there is no missing data.\n4.For the f277w filter, the missing data percentage is less at higher redshifts.\nSo, how does the missing data increase or decrease with redshift?\n\n\nCode\nmiss_cont &lt;- miss_long %&gt;%\n  mutate(\n    z_mid = recode(as.character(z_bin),\n      \"[0,1)\" = 0.5, \"[1,2)\" = 1.5, \"[2,3)\" = 2.5,\n      \"[3,4)\" = 3.5, \"[4,6)\" = 5,   \"[6,10)\" = 8,\n      \"[10,+)\" = 11\n    )\n  )\n\nbrks &lt;- c(0,2,4,6,8,10,12)\n\nggplot(miss_cont, aes(x = z_mid, y = pct_missing, group = 1)) +\n  geom_line(linewidth = 1) +\n  geom_point(size = 1.8) +\n  facet_wrap(~ band, ncol = 2) +\n  scale_x_continuous(breaks = brks, limits = c(0, 12), minor_breaks = NULL) +\n  labs(title = \"Missing data across redshift, by band\",\n       x = \"Redshift (z)\", y = \"% Missing\") +\n  theme_minimal() +\n  theme(panel.spacing = unit(12, \"pt\"),\n        axis.text.x = element_text(angle = 40, hjust = 1),\n        strip.text  = element_text(face = \"bold\"))\n\n\n\n\n\n\n\n\n\nObservations:\n\nI can more clearly observe here that at lower values of redshift for a shorter wavelength filters (F150W and F200W), the missing data is modest. However as the redshift becomes very high (10+), these short wavelength filters, that are looking at bluer rest-frame light and is thus suppressing them. So many galaxies are not getting detected.\nIn longer wavelength bands (F444W), the rise of missing data with redshift is minimal because longer wavelength filters are more effective and usable. As we travel to older cosmic times, the bluer wavelength filters drop out lots of undetectable points while the redder wavelength filters stay on longer.\n\nTherefore, based on the above two plots, I can conclude that the low + no impact filters are perfect for plotting/modelling.\nFor Data Wrangling:\n\n\nCode\n#Standardizing redshift\nz_candidates &lt;- c(\"z\", \"z_phot\", \"z_best\", \"z_eazy\", \"photoz\", \"z_spec\")\nz_col &lt;- intersect(z_candidates, names(gal_raw))[1]\nif (is.na(z_col)) {\n  stop(\"No redshift column found. Look for one of: \",\n       paste(z_candidates, collapse = \", \"))\n}\nmessage(\"Using redshift column: \", z_col)\ngal &lt;- gal_raw %&gt;%\n  mutate(\n    z = as.numeric(.data[[z_col]]),\n    z = ifelse(is.finite(z) & z &gt;= 0, z, NA_real_),\n    z_bin = cut(\n      z,\n      breaks = c(0, 1, 2, 3, 4, 6, 10, Inf),\n      labels = c(\"[0,1)\",\"[1,2)\",\"[2,3)\",\"[3,4)\",\"[4,6)\",\"[6,10)\",\"[10,+)\"),\n      right  = FALSE\n    )\n  )\n\n#Magnitude for filter bands\nmag_from_uJy &lt;- function(f) {\n  f &lt;- suppressWarnings(as.numeric(f))\n  out &lt;- rep(NA_real_, length(f))\n  idx &lt;- is.finite(f) & f &gt; 0\n  out[idx] &lt;- 23.9 - 2.5 * log10(f[idx])\n  out\n}\n\nbands &lt;- c(\"f150w\",\"f200w\",\"f277w\",\"f356w\",\"f360w\",\n           \"f410w\",\"f430w\",\"f444w\",\"f460w\",\"f480w\")\n\nfor (b in bands) {\n  fcol &lt;- paste0(\"f_\", b)\n  mcol &lt;- paste0(\"m_\", b)\n  if (fcol %in% names(gal) && !(mcol %in% names(gal))) {\n    gal[[mcol]] &lt;- mag_from_uJy(gal[[fcol]])\n  }\n}\n\nmag_cols &lt;- intersect(paste0(\"m_\", bands), names(gal))\n\n#Choosing the optimal filters from missing data analysis\nsafe_filters_target &lt;- c(\"F277W\", \"F356W\", \"F444W\", \"F480W\")\n\n#Column check\ncandidate_safe_mag_cols &lt;- paste0(\"m_\", tolower(safe_filters_target))\nsafe_mag_cols &lt;- intersect(candidate_safe_mag_cols, names(gal))\n\nif (length(safe_mag_cols) == 0) {\n  stop(\"None of the target safe filters are present as magnitude columns:\",\n       paste(safe_filters_target, collapse = \", \"))\n}\n\nsafe_filters &lt;- toupper(sub(\"^m_f\", \"F\", safe_mag_cols))\n\nmessage(\"Using safe filters: \", paste(safe_filters, collapse = \", \"))\n\n#Clean analysis df\ngal_clean &lt;- gal %&gt;%\n  mutate(\n    n_safe   = rowSums(across(all_of(safe_mag_cols), ~ is.finite(.))),\n    has_safe = n_safe &gt; 0\n  ) %&gt;%\n  filter(has_safe, !is.na(z))\n\ngal_clean &lt;- gal_clean %&gt;%\n  mutate(\n    C1_F277_F356 = m_f277w - m_f356w,\n    C2_F356_F444 = m_f356w - m_f444w\n  )\n\n#Long format magnitudes\ngal_long_mag &lt;- gal_clean %&gt;%\n  select(any_of(c(\"id\", \"tile\", \"class\")), z, z_bin, all_of(safe_mag_cols)) %&gt;%\n  pivot_longer(\n    cols      = all_of(safe_mag_cols),\n    names_to  = \"band_raw\",\n    values_to = \"mag\"\n  ) %&gt;%\n  mutate(\n    band = toupper(sub(\"^m_f\", \"F\", band_raw))\n  )\n\n#Visible output\n\nwrangle_summary &lt;- dplyr::tibble(\n  stage  = c(\"Raw catalog\", \"After z + mags\", \"After safe-filter & z filter\"),\n  n_rows = c(nrow(gal_raw), nrow(gal), nrow(gal_clean))\n)\n\nknitr::kable(\n  wrangle_summary,\n  caption = \"Row counts at each data-wrangling stage\"\n)\n\n\n\nRow counts at each data-wrangling stage\n\n\nstage\nn_rows\n\n\n\n\nRaw catalog\n74020\n\n\nAfter z + mags\n74020\n\n\nAfter safe-filter & z filter\n71256\n\n\n\n\n\nCode\nsafe_filter_tbl &lt;- dplyr::tibble(\n  safe_filters   = paste(safe_filters, collapse = \", \"),\n  n_safe_filters = length(safe_filters)\n)\n\nknitr::kable(\n  safe_filter_tbl,\n  caption = \"Safe / low-missing filters available in this catalog\"\n)\n\n\n\nSafe / low-missing filters available in this catalog\n\n\nsafe_filters\nn_safe_filters\n\n\n\n\nF277W, F356W, F444W\n1\n\n\n\n\n\nCode\n#Glimpse of the cleaned table for the used filters\ngal_clean %&gt;%\n  dplyr::select(z, z_bin, all_of(safe_mag_cols)) %&gt;%\n  head(8)\n\n\n# A tibble: 8 √ó 5\n      z z_bin  m_f277w m_f356w m_f444w\n  &lt;dbl&gt; &lt;fct&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 0.982 [0,1)     24.5    24.1    24.0\n2 1.86  [1,2)     24.8    24.8    24.9\n3 5.15  [4,6)     24.6    25.4    25.2\n4 0.173 [0,1)     23.6    23.5    23.7\n5 5.66  [4,6)     25.2    24.2    24.7\n6 6.59  [6,10)    24.0    23.5    24.0\n7 2.30  [2,3)     22.7    22.6    22.5\n8 0.427 [0,1)     22.7    22.9    23.1\n\n\n\n\nCode\ndir.create(\"data/clean\", recursive = TRUE, showWarnings = FALSE)\n\nsaveRDS(\n  list(\n    gal_clean     = gal_clean,\n    gal_long_mag  = gal_long_mag,\n    safe_mag_cols = safe_mag_cols\n  ),\n  \"data/clean/gal_clean.rds\"\n)",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "results.html",
    "href": "results.html",
    "title": "3¬† Results",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(viridis)\nlibrary(hexbin)\nlibrary(naniar)\nlibrary(ggridges)\nlibrary(scales)\nlibrary(naniar)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(stringr)\nlibrary(hexbin)\nlibrary(broom)\nlibrary(tibble)\n\n\n\n\nCode\nclean_obj &lt;- readRDS(\"data/clean/gal_clean.rds\")\n\ngal_clean     &lt;- clean_obj$gal_clean\ngal_long_mag  &lt;- clean_obj$gal_long_mag\nsafe_mag_cols &lt;- clean_obj$safe_mag_cols\n\n\n(Using F356W as anchor band)\nI. Does brightness change with time?\nI will use two different plots for my analysis.\nCode 1: Dot Density Plot\n\n\nCode\n#Using 1 safe band for brightness\nband_brightness &lt;- \"F356W\"\nmag_col &lt;- paste0(\"m_\", tolower(band_brightness))\n\ngal_bright &lt;- gal_clean %&gt;%\n  filter(is.finite(.data[[mag_col]])) %&gt;%\n  mutate(mag = .data[[mag_col]])\n\n#Dot density scatter sample\nn_sample &lt;- min(25000L, nrow(gal_bright))\ngal_bright_sample &lt;- gal_bright %&gt;%\n  slice_sample(n = n_sample)\n\n#Median magnitude per redshift bin + midpoints\nmed_brightness &lt;- gal_bright %&gt;%\n  group_by(z_bin) %&gt;%\n  summarise(\n    med_mag = median(mag, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(\n    z_mid = dplyr::recode(as.character(z_bin),\n      \"[0,1)\" = 0.5, \"[1,2)\" = 1.5, \"[2,3)\" = 2.5,\n      \"[3,4)\" = 3.5, \"[4,6)\" = 5,   \"[6,10)\" = 8,\n      \"[10,+)\" = 11\n    )\n  ) %&gt;%\n  filter(!is.na(z_mid))\n#Plot\nggplot(gal_bright_sample, aes(x = z, y = mag)) +\n  geom_point(color = \"navy\", alpha = 0.3, size = 0.4) +\n  geom_line(data = med_brightness,\n            aes(x = z_mid, y = med_mag),\n            colour = \"orange2\", linewidth = 1.1) +\n  scale_y_reverse() +\n  labs(\n    title    = \"Observed brightness vs redshift (F356W)\",\n    subtitle = \"Dot-density scatter with median apparent magnitude by redshift bin\",\n    x = \"Redshift (z)\",\n    y = \"Apparent magnitude (AB; lower = brighter)\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode 2: Hexbin Density Plot\n\n\nCode\nggplot(gal_bright, aes(x = z, y = mag)) +\n  stat_binhex(bins = 60, alpha = 3.5) +\n  scale_y_reverse() +\n  scale_fill_distiller(\n    palette   = \"Blues\",\n    direction = 1,\n    trans     = \"log10\", #log stretch so both sparse & dense regions show up\n    name      = \"Count\"\n  ) +\n  labs(\n    title    = \"Observed brightness vs redshift (F356W)\",\n    subtitle = \"Hexbin density with sequential blue count scale\",\n    x = \"Redshift (z)\",\n    y = \"Apparent magnitude (AB; lower = brighter)\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nObservations:\n\nThe dot density plot shows the density spread from bright points in the galaxies to low, faint spots in the galaxies. The median line is consistent around 23-24 AB magnitude until z=10.\nTo get a better clarity, the hexbin density plot further defines the visualization. At lower z, there is a greater density of different mix of stars, brighter galaxies, fainter galaxies, etc. Around z=10 or higher redshifts, I can clearly observe that only the brightest galaxies are observable.\nAt very high redshifts like z&gt;15, the survey starts reaching it‚Äôs limits. Only a few sources detected.\n\n\nDo galaxies evolve with time?\n\n\n\nCode\n# Bin brightness in F356W (you can tweak breaks)\ngal_ev &lt;- gal_clean %&gt;%\n  \nfilter(\nis.finite(z),\nis.finite(m_f356w),\nis.finite(C2_F356_F444)\n)\nif (\"class\" %in% names(gal_ev)) {\ngal_ev &lt;- gal_ev %&gt;%\nfilter(str_detect(tolower(class), \"gal\")) # matches \"galaxy\", \"gal\", etc.\n}\ngal_ev &lt;- gal_ev %&gt;%\nfilter(\nz &gt;= 0, z &lt;= 12,\nm_f356w &gt; 18, m_f356w &lt; 32 # drop crazy outliers\n)\ngal_ev %&gt;%\nselect(z, z_bin, m_f356w, m_f444w, C2_F356_F444) %&gt;%\nslice_head(n = 10)\n\n\n# A tibble: 10 √ó 5\n       z z_bin  m_f356w m_f444w C2_F356_F444\n   &lt;dbl&gt; &lt;fct&gt;    &lt;dbl&gt;   &lt;dbl&gt;        &lt;dbl&gt;\n 1 0.982 [0,1)     24.1    24.0       0.0411\n 2 1.86  [1,2)     24.8    24.9      -0.0913\n 3 5.15  [4,6)     25.4    25.2       0.164 \n 4 0.173 [0,1)     23.5    23.7      -0.169 \n 5 5.66  [4,6)     24.2    24.7      -0.473 \n 6 6.59  [6,10)    23.5    24.0      -0.497 \n 7 2.30  [2,3)     22.6    22.5       0.144 \n 8 0.427 [0,1)     22.9    23.1      -0.117 \n 9 6.72  [6,10)    22.8    24.4      -1.64  \n10 6.69  [6,10)    24.0    24.9      -0.892 \n\n\n\n\nCode\nggplot(gal_ev, aes(x = z, y = m_f356w)) +\nstat_summary_hex(\naes(z = C2_F356_F444),\nbins = 40, alpha=3.5,\nfun = median\n) +\nscale_y_reverse() +\nscale_fill_distiller(\ntype = \"div\",\npalette = \"RdBu\",\ndirection = -1, # blue = negative (bluer), red = positive (redder)\nna.value = \"grey90\",\nname = \"Median colour\\n(F356W ‚àí F444W)\"\n) +\nlabs(\ntitle = \"Do galaxies evolve with time?\",\nsubtitle = \"Redshift vs F356W magnitude, coloured by median F356W ‚àí F444W\",\nx = \"Redshift z (proxy for lookback time)\",\ny = \"F356W apparent magnitude (AB, brighter at top)\"\n) +\ntheme_minimal(base_size = 11) +\ntheme(\npanel.grid.minor = element_blank(),\nlegend.position = \"right\"\n)\n\n\n\n\n\n\n\n\n\n\n\nCode\ncol_z_trend &lt;- gal_ev %&gt;%\nfilter(!is.na(z_bin)) %&gt;%\ngroup_by(z_bin) %&gt;%\nsummarise(\nn = n(),\nmed_col = median(C2_F356_F444, na.rm = TRUE),\nq25 = quantile(C2_F356_F444, 0.25, na.rm = TRUE),\nq75 = quantile(C2_F356_F444, 0.75, na.rm = TRUE),\n.groups = \"drop\"\n) %&gt;%\n\nmutate(\nz_mid = recode(as.character(z_bin),\n\"[0,1)\" = 0.5, \"[1,2)\" = 1.5, \"[2,3)\" = 2.5,\n\"[3,4)\" = 3.5, \"[4,6)\" = 5, \"[6,10)\" = 8,\n\"[10,+)\" = 11\n)\n) %&gt;%\nfilter(n &gt;= 30) # drop super-sparse bins\n\nggplot(col_z_trend, aes(x = z_mid, y = med_col)) +\ngeom_ribbon(\naes(ymin = q25, ymax = q75),\nfill = \"grey85\", alpha = 0.7\n) +\ngeom_line(linewidth = 0.9) +\ngeom_point(size = 2) +\ngeom_hline(yintercept = 0, linetype = \"dashed\", colour = \"grey40\") +\nscale_x_continuous(\nbreaks = c(0.5, 1.5, 2.5, 3.5, 5, 8, 11),\nlabels = c(\"[0,1)\", \"[1,2)\", \"[2,3)\", \"[3,4)\", \"[4,6)\", \"[6,10)\", \"[10,+)\")\n) +\nlabs(\ntitle = \"Median galaxy colour vs cosmic time\",\nsubtitle = \"F356W ‚àí F444W in redshift bins (line = median, band = IQR)\",\nx = \"Redshift bin (z)\",\ny = \"Colour (F356W ‚àí F444W)\"\n) +\ntheme_minimal(base_size = 11) +\ntheme(\naxis.text.x = element_text(angle = 40, hjust = 1)\n)\n\n\n\n\n\n\n\n\n\nObservations:\nIt can be observed that the galaxy colors in the observed sample vary across redshift, but not in a clean way. In the median color vs.¬†redshift plot, the median (F356W‚ÄìF444W) fluctuates around 0 and only becomes slightly positive in some bins, so I cannot claim a steady ‚Äúredder with time‚Äù trend. The hexbin plot suggests a clearer pattern with brightness: the reddest colours concentrate mostly among the faintest galaxies (higher F356W magnitude), across multiple redshifts. Since high-redshift galaxies are typically faint in brightness, and closer to the survey limit, this can create an apparent ‚Äúredshift‚Äìcolour‚Äù effect. Therefore, based on my results, the evidence for true galaxy evolution from these plots alone is inconclusive.\nTo know more about the survey, I will move on to a spatial non uniformity analysis.\n\nSpatial Non Uniformity\n\n\n\nCode\nra_min &lt;- min(gal_clean$ra, na.rm = TRUE)\nra_max &lt;- max(gal_clean$ra, na.rm = TRUE)\ndec_min &lt;- min(gal_clean$dec, na.rm = TRUE)\ndec_max &lt;- max(gal_clean$dec, na.rm = TRUE)\n\nn_ra &lt;- 12 #number of tiles in RA direction\nn_dec &lt;- 10 #number of tiles in Dec direction\n\nra_step &lt;- (ra_max - ra_min) / n_ra\ndec_step &lt;- (dec_max - dec_min) / n_dec\n\ngal_tiles &lt;- gal_clean %&gt;%\nmutate(\nra_tile = floor((ra - ra_min) / ra_step),\ndec_tile = floor((dec - dec_min) / dec_step),\nra_tile = pmin(pmax(ra_tile, 0L), n_ra - 1L),\ndec_tile = pmin(pmax(dec_tile, 0L), n_dec - 1L),\nra_ctr = ra_min + (ra_tile + 0.5) * ra_step,\ndec_ctr = dec_min + (dec_tile + 0.5) * dec_step\n)\n\ntile_stats &lt;- gal_tiles %&gt;%\ngroup_by(ra_tile, dec_tile, ra_ctr, dec_ctr) %&gt;%\nsummarise(\nn_obj = n(),\nmed_mag = median(m_f356w, na.rm = TRUE),\nmed_color = median(C2_F356_F444, na.rm = TRUE),\nfrac_safe = mean(rowSums(across(all_of(safe_mag_cols), ~ is.finite(.))) &gt; 0),\n.groups = \"drop\"\n)\n\ntile_stats\n\n\n# A tibble: 75 √ó 8\n   ra_tile dec_tile ra_ctr dec_ctr n_obj med_mag med_color frac_safe\n     &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1       0        6   3.45   -30.4     2    21.9   -0.472          1\n 2       0        7   3.45   -30.3   966    23.7   -0.0615         1\n 3       0        8   3.45   -30.3   649    23.6   -0.0255         1\n 4       1        6   3.47   -30.4   497    23.8   -0.0208         1\n 5       1        7   3.47   -30.3  1588    24.0   -0.0394         1\n 6       1        8   3.47   -30.3  1596    23.9   -0.0125         1\n 7       1        9   3.47   -30.3   452    23.6   -0.0903         1\n 8       2        4   3.49   -30.4    39    23.3   -0.0580         1\n 9       2        5   3.49   -30.4   742    23.6   -0.0644         1\n10       2        6   3.49   -30.4   898    23.7   -0.134          1\n# ‚Ñπ 65 more rows\n\n\na)Median F356W magnitude (depth proxy)\n\n\nCode\np_depth &lt;- ggplot(tile_stats,\naes(x = ra_ctr, y = dec_ctr, fill = med_mag)) +\ngeom_tile(colour = \"grey30\", linewidth = 0.2) +\nscale_fill_distiller(\npalette = \"PuBu\",\ndirection = -1,\nna.value = \"grey90\",\nname = \"Median\\nF356W mag\"\n) +\nscale_y_continuous(expand = expansion(mult = 0.02)) +\nscale_x_continuous(expand = expansion(mult = 0.02)) +\ncoord_equal() +\nlabs(\ntitle = \"Spatial non-uniformity in survey depth\",\nx = \"Right ascension (deg)\",\ny = \"Declination (deg)\"\n) +\ntheme_minimal()\np_depth\n\n\n\n\n\n\n\n\n\nb)Median Color\n\n\nCode\np_color &lt;- ggplot(tile_stats,\naes(x = ra_ctr, y = dec_ctr, fill = med_color)) +\ngeom_tile(colour = \"grey30\", linewidth = 0.2) +\nscale_fill_distiller(\npalette = \"RdBu\",\ndirection = -1,\nname = \"Median\\nF356W ‚Äì F444W\"\n) +\nscale_y_continuous(expand = expansion(mult = 0.02)) +\nscale_x_continuous(expand = expansion(mult = 0.02)) +\ncoord_equal() +\nlabs(\ntitle = \"Spatial pattern in median galaxy colour\",\nx = \"Right ascension (deg)\",\ny = \"Declination (deg)\"\n) +\ntheme_minimal()\np_color\n\n\n\n\n\n\n\n\n\nIt can be easily observed that the survey is not uniform in the sky. Some patches were observed deeper, while some patches were observed shallower. In the depth proxy plot, I am trying to measure the brightness in a tile, and whether we‚Äôre detecting fainter spots in the deeper tiles and brighter spots in the shallower tiles. For the median color plot, I used a diverging color scheme to show how much the colors of the galaxy vary spatially, where neutrals at 0 are at white tiles.\nBased on the above plots, when I try to analyze if deeper tiles have differen median colors, it can be somewhat observed that the effects of survey bias plays a string role. Because survey depth varies across the field, and tiles with different depth show different median colours, some of the observed colour pattern could reflect survey effects rather than pure cosmic evolution.\n\nCan colors and size predict star vs galaxy?\n\nPrepare modelling data:\n\n\nCode\n# 1. Prepare modelling data\nsg_data &lt;- gal_clean %&gt;%\n  mutate(\n    # SNR in F356W from flux / error (only where both are finite)\n    snr_f356w = ifelse(\n      is.finite(f_f356w) & is.finite(e_f356w) & e_f356w &gt; 0,\n      f_f356w / e_f356w,\n      NA_real_\n    )\n  ) %&gt;%\n  filter(\n    flag_star %in% c(0, 1),                  # only objects with star flag defined\n    is.finite(C1_F277_F356),\n    is.finite(C2_F356_F444),\n    is.finite(m_f356w),\n    is.finite(kron_radius_circ),\n    is.finite(snr_f356w),\n    snr_f356w &gt;= 5                           # S/N cut to avoid very noisy cases\n  ) %&gt;%\n  mutate(\n    is_star       = flag_star == 1L,         # logical\n    y_true        = as.integer(is_star),     # 1 = star, 0 = galaxy/non-star\n    true_class    = if_else(is_star, \"Star\", \"Galaxy\"),\n    true_class    = factor(true_class, levels = c(\"Galaxy\", \"Star\"))\n  )\n\n# Logistic regression: colours + magnitude + size\nfit_logit &lt;- glm(\n  y_true ~ C1_F277_F356 + C2_F356_F444 + m_f356w + kron_radius_circ,\n  data   = sg_data,\n  family = binomial()\n)\n\n# Coefficient table with odds ratios\nlogit_coef &lt;- tidy(fit_logit) %&gt;%\n  mutate(odds_ratio = exp(estimate))\n\nlogit_coef\n\n\n# A tibble: 5 √ó 6\n  term             estimate std.error statistic  p.value odds_ratio\n  &lt;chr&gt;               &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;\n1 (Intercept)       31.5        2.68    11.8    5.07e-32   4.98e+13\n2 C1_F277_F356      -0.0551     0.562   -0.0982 9.22e- 1   9.46e- 1\n3 C2_F356_F444      -0.753      0.368   -2.05   4.08e- 2   4.71e- 1\n4 m_f356w           -1.53       0.117  -13.2    1.70e-39   2.16e- 1\n5 kron_radius_circ -14.3        1.61    -8.91   4.94e-19   5.91e- 7\n\n\nModel: logit{ùëÉ(star)}=ùõΩ0+ùõΩ1ùê∂1+ùõΩ2ùê∂2+ùõΩ3ùëö356+ùõΩ4kron_radius with y_true = 1 for stars (flag_star == 1) and y_true = 0 for galaxies/non-stars. Predictors: C1_F277_F356, C2_F356_F444, m_f356w, kron_radius_circ, S/N cut: snr_f356w ‚â• 5\nObservations:\n\nThe model tells us which measurements separate stars from galaxies.\nAs the value of magnitude increases, objects get fainter (astronomical brightness is high for lower magnitude values). Therefore the chances of an object being a star drop.\nRedder color may hint at lesser chances of the object being a star. (C2)\nSize is measured by kron_radius_circ. If the objects get spread out, they‚Äôre more likely galaxies. Stars are pointy while galaxies are spread out.\n\nModel Diagnostics:\n\n\nCode\nsg_data &lt;- sg_data %&gt;%\n  mutate(\n    p_star = predict(fit_logit, type = \"response\"),\n    y_hat  = if_else(p_star &gt;= 0.5, 1L, 0L)\n  )\n\nn_total &lt;- nrow(sg_data)\n\n# Misclassification rate of logistic model\nerr_rate &lt;- mean(sg_data$y_hat != sg_data$y_true)\n\n# Null classifier: always predict majority class\nmajority_class &lt;- as.integer(names(which.max(table(sg_data$y_true))))\nnull_err &lt;- mean(rep(majority_class, n_total) != sg_data$y_true)\n\n# McFadden pseudo-R¬≤ (EDAV-style: using deviances)\npseudo_R2 &lt;- 1 - (fit_logit$deviance / fit_logit$null.deviance)\n\nlogit_diag &lt;- tibble(\n  metric = c(\n    \"Error rate (logistic)\",\n    \"Error rate (null classifier)\",\n    \"McFadden pseudo-R¬≤\"\n  ),\n  value = c(err_rate, null_err, pseudo_R2)\n)\n\nlogit_diag\n\n\n# A tibble: 3 √ó 2\n  metric                         value\n  &lt;chr&gt;                          &lt;dbl&gt;\n1 Error rate (logistic)        0.00106\n2 Error rate (null classifier) 0.00114\n3 McFadden pseudo-R¬≤           0.563  \n\n\nObservations: The logistic error rate is slightly better than the null classifier. This means that our dataset has very few stars. Since it is a rare class problem, our metric is not sensitive enough for determining this. The McFadden pseudo R^2 shows that although stars are rare, there is a clear separation of classes between stars and galaxies. So, in likelihood terms the model is not weak.\n\n\nCode\nsg_data &lt;- sg_data %&gt;%\n  mutate(\n    is_star    = flag_star == 1L,                      # or your existing definition\n    true_class = if_else(is_star, \"Star\", \"Galaxy\"),\n    p_star     = predict(fit_logit, type = \"response\") # predicted P(star)\n  )\n\n# Plot: predicted P(star) vs colour C2\nggplot(sg_data,\n       aes(x = C2_F356_F444, y = p_star, colour = true_class)) +\n  geom_point(alpha = 0.3, size = 0.7) +\n  # binned average predicted probability across C2\n  stat_summary_bin(\n    aes(x = C2_F356_F444, y = p_star),\n    bins        = 20,\n    fun         = mean,\n    geom        = \"line\",\n    colour      = \"black\",\n    linewidth   = 0.8,\n    inherit.aes = FALSE\n  ) +\n  labs(\n    title    = \"Logistic regression: predicted P(star) vs colour C2\",\n    subtitle = \"Outcome: is_star; predictors: colours, magnitude, size\",\n    x        = \"Colour C2 = F356W ‚àí F444W (mag)\",\n    y        = \"Predicted probability P(star)\",\n    colour   = \"True class\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThe model shows that star predictions are very close to 0. This is natural, since the data is galaxy dominated in the galaxy cluster. At C2=0, there is a vertical stack of star vs galaxies points which shows that the model does not separate based on color alone. At C2=-5, there seems to be some outliers from limited samples.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Results</span>"
    ]
  },
  {
    "objectID": "conclusion.html",
    "href": "conclusion.html",
    "title": "4¬† Conclusion",
    "section": "",
    "text": "Overall, I would conclude from this experiment on the JWST photometric data that from our brightness analysis, the magnitude stays stable. But at higher redshift, only the brightest sources stay detectable. This may indicate a survey limit. From a visualization perspective, color trend does not show a redder with time trend, which means there is more survey bias rather than actual cosmic evolution. The spatial choropleths further strengthen the point that the survey itself is non uniform. The logistic regression, highly concludes that the star galaxy identification is highly driven by brightness and size.\nRESOURCES:\n\nhttps://jwst-uncover.github.io/DR3.html\nhttps://mast.stsci.edu/portal/Mashup/Clients/Mast/Portal.html\nIdea derived from https://medium.com/cosmic-cartography/data-visualization-of-the-observable-universe-5e4a5af2330\nChatgpt for R\nAstropy hdu=1 for converting fits file to csv and ecsv.\nhttps://pixedfit.readthedocs.io/en/latest/list_kernels_psf.html\nhttps://science.nasa.gov/asset/webb/pandoras-cluster-nircam-image/",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Conclusion</span>"
    ]
  }
]